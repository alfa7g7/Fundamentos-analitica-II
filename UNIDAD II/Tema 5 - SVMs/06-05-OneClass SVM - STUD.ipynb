{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Class SVMs: detección de anomalías"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay dos maneras de detectar anomalías:\n",
    "\n",
    "- De manera supervisada: se tienen suficientes instancias normales e instancias anormales del evento o entidad a analizar y por lo tanto se puede entrenar un modelo de clasificación.\n",
    "- De manera no supervisada: solo se tienen instancias del evento o entidad a analizar, por lo que se debe analizar el patrón de lo que es común y de lo que no sería común.\n",
    "\n",
    "Los modelos One-Class SVM pertenecen al segundo caso, identificando regiones del espacio de representación de las instancias que se consideran típicas de la categoría de interés, y por oposición regiones que serían atípicas para instancias de la misma.\n",
    "\n",
    "Siguiendo con la idea de base de los SVMs, se define entonces una distribución de tipicidad de las instancias de la categoría. Para poder representar distribuciones de mayor riqueza, se utilizan kernels para proyectar los datos de entrenamiento a un espacio de mayor dimensionalidad, y para computar la similitud de cada nueva instancia a analizar con instancias que se encuentran en el límite de lo que se considera anormal sin tener que hacer un cambio explícito en la representación de los datos (\"*kernel trick*\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este taller está inspirado de un ejemplo de scikit-learn: https://scikit-learn.org/0.15/_downloads/plot_oneclass.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creación de datasets sintéticos aleatorios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos las librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager\n",
    "import seaborn as sns\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#warnings.filterwarnings(action=‘ignore’,category=DeprecationWarning)\n",
    "#warnings.filterwarnings(action=‘ignore’,category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a crear tres datasets aleatorios en un espacio de 2 dimensiones:\n",
    "\n",
    "- dataset de entrenamiento: sobre el que se entrenará el modelo, utilizando un proceso generativo bimodal determinado\n",
    "- dataset de test: se genera utilizando el mismo proceso generativo que el de entrenamiento\n",
    "- dataset de outliers: siguiendo otro proceso generativo diferente al de los otros 2 datasets\n",
    "\n",
    "El modelo aprendido con el primer dataset se aplicará a los otros 2 datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero generamos 100 puntos de 2 dimensiones aleatoiramente siguiendo una distribucipon normal estándar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(17)\n",
    "X = 0.3* np.random.randn(100, 2)\n",
    "print(X.shape)\n",
    "X[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos originales aleatorios no los vamos a conservar.\n",
    "Con el método **numpy.r_** podemos aumentarlos, definiendo copias trasladadas del mismo.\n",
    "En el dataset de entrenamiento tendremos 2 copias del dataset anterior, una primera con 2 unidades restadas, otra segunda con 2 unidades adicionadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.r_[X+2, X-2]\n",
    "X_train[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repetimos el proceso para crear el set de test con 20 instancias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = 0.3*np.random.randn(20, 2)\n",
    "X_test = np.r_[X + 2, X - 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataset de outliers lo creamos siguiendo una distribución diferente (uniforme), con valores que caen en el mismo rango de valores que los datasets de entrenamiento y de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_outliers = np.random.uniform(low=-4, high=4, size=(20, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ploteamos los datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter([a[0] for a in X_train],[a[1] for a in X_train], c=\"g\", label=\"Training\")\n",
    "plt.scatter([a[0] for a in X_test],[a[1] for a in X_test], c=\"y\", label=\"Test\")\n",
    "plt.scatter([a[0] for a in X_outliers],[a[1] for a in X_outliers], c=\"r\", label=\"Anomalías\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que en efecto los puntos de entrenamiento y de test siguen una distribución similar, bien diferente a la de los puntos anómalos. Sin embargo, esto no previene que no vayamos a confundir algunas de las anomalías como puntos normales "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Entrenamiento de modelo One-Class SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a utilizar la clase **OneClassSVM**, del paquete scikit-learn.\n",
    "Para la instanciación, hay que especificar los mismos parámetros que con un clasificador o un regresor, aunque no hay que indicar el valor de *C*, pues no tiene sentido hablar de error de instancia en este caso. Nos interesan particularmente:\n",
    "\n",
    "- *kernel*: el tipo de kernel a utilizar ('rbf' por defecto); otros valores aceptados son: ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’. El kernel con más sentido desde el punto de vista de detección de anomalías es el rbf.\n",
    "- *degree*: grado del polinomio en el caso del kernel 'poly' (3 por defecto); se ignora para otros kernels.\n",
    "- *gamma*: determina la dispersión alrededor de los vectores de soporte ('auto' por defecto); solo se considera para los kernels ‘rbf’, ‘poly’ y ‘sigmoid’. Por defecto, *gamma* es igual a 1/m, donde m es el número de dimensiones. A mayor *gamma*, más complejo será el espacio de representación de los datos.\n",
    "\n",
    "Se tiene además el siguiente parámetro, específico del One-Class SVM:\n",
    "\n",
    "- *nu*: máxima la proporción de anomalías en el set de entrenamiento, tal que $0<nu\\leq 1$ (por defecto 0.5, por lo que, a lo sumo, la mitad de los puntos de entrenamiento podrán considerarse outliers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = OneClassSVM()\n",
    "clf.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos una función que nos permitirá analizar cómo queda la representación de la frontera de decisión que define el límite entre los puntos comúnes y los outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_oneclass_svm(svm):\n",
    "    # Definimos una grilla de puntos sobre la cual vamos a determinar la frontera de detección de anomalías:\n",
    "    xx, yy = np.meshgrid(np.linspace(-5, 5, 500), np.linspace(-5, 5, 500))\n",
    "\n",
    "    # Obtenemos la distancia con la frontera de decisión para cada punto\n",
    "    Z = svm.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.title(\"Fronteras de detección de anomalías (en rojo)\")\n",
    "    \n",
    "    # Ploteamos fronteras y pintamos regiones interna y externa a la frontera\n",
    "    plt.contourf(xx, yy, Z, levels=[Z.min(), 0], colors=\"gray\") # Región anómala\n",
    "    a = plt.contour(xx, yy, Z, levels=[0], linewidths=4, colors='red') # Fronteras de decisión\n",
    "    plt.contourf(xx, yy, Z, levels=[0, Z.max()], colors='palevioletred') # Región de tipicidad\n",
    "    \n",
    "    # Ploteamos los puntos de entrenamiento, test y anomalías\n",
    "    s = 40\n",
    "    b1 = plt.scatter(X_train[:, 0], X_train[:, 1], s=s, edgecolors='k', c=\"g\") # Puntos de entrenamiento\n",
    "    b2 = plt.scatter(X_test[:, 0], X_test[:, 1], s=s, edgecolors='k', c=\"y\") # Puntos de Test\n",
    "    c = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], s=s, edgecolors='k', c=\"r\") # Puntos excepcionales\n",
    "    \n",
    "    #Leyenda\n",
    "    plt.axis('tight') # Solo el espacio necesario\n",
    "    plt.xlim((-5, 5))\n",
    "    plt.ylim((-5, 5))\n",
    "    plt.legend([a.collections[0], b1, b2, c],\n",
    "               [\"Frontera de anomalías\", \"Training\", \"Test normales\", \"Test anómalos\"],\n",
    "               loc=\"upper left\",\n",
    "               prop=matplotlib.font_manager.FontProperties(size=11))\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculamos accuracy del training, test positivos y negativos\n",
    "    y_pred_train = clf.predict(X_train)\n",
    "    y_pred_test = clf.predict(X_test)\n",
    "    y_pred_outliers = clf.predict(X_outliers)\n",
    "    n_error_train = y_pred_train[y_pred_train == -1].size\n",
    "    n_error_test = y_pred_test[y_pred_test == -1].size\n",
    "    n_error_outliers = y_pred_outliers[y_pred_outliers == 1].size\n",
    "    \n",
    "    print(\"Accuracy del training set: \"+str(1-n_error_train/len(X_train)))\n",
    "    print(\"Recall (normales) del test set: \"+str(1-n_error_test/len(X_test)))\n",
    "    print(\"Especificidad (anomalías) del test set: \"+str(1-n_error_outliers/len(X_outliers)))\n",
    "    print(\"Accuracy del test set entero: \"+ str(1-(n_error_test+n_error_outliers)/(len(X_test)+len(X_outliers))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_oneclass_svm(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver en el plot, y en la baja *accuracy*, la región de tipicidad de las instancias es demasiado conservadora, ya que permitimos que la mitad de los datos de entrenamiento sean considerados como outliers. Vamos a reducir *nu*, para aumentar la región de tipicidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = OneClassSVM(nu=0.1)\n",
    "clf.fit(X_train)\n",
    "plot_oneclass_svm(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver cómo las areas de tipicidad aumentan y la exactitud de la detección de outliers aumenta considerablemente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a cambiar ahora el valor de *gamma*, aumentándo la flexibilidad de las fronteras de detección de las anomalías."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = OneClassSVM(nu=0.1,gamma=10)\n",
    "clf.fit(X_train)\n",
    "plot_oneclass_svm(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí vemos demasiada complejidad, con un modelo con un claro overfitting, por lo que la exactitud baja con repecto a la configuración anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si por el contrario reducimos el gamma a 0.0001, vamos a tener el efecto contrario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = OneClassSVM(nu=0.1,gamma=0.0001)\n",
    "clf.fit(X_train)\n",
    "plot_oneclass_svm(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora la frontera se simplificó demasiado, dañanado por completo el modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intentemos finalmente con kernel polinomial de grado 2 y de grado 20:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = OneClassSVM(kernel=\"poly\", degree=2,nu=0.1)\n",
    "clf.fit(X_train)\n",
    "plot_oneclass_svm(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = OneClassSVM(kernel=\"poly\", degree=20,nu=0.1)\n",
    "clf.fit(X_train)\n",
    "plot_oneclass_svm(clf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
