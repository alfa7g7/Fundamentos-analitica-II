{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Fundamentos-analitica-II**\n",
    "FACULTAD DE INGENIERÍA, DISEÑO Y CIENCIAS APLICADAS\n",
    " \n",
    "MAESTRÍA EN CIENCIA DE DATOS\n",
    "\n",
    "TIC 60153 – Fundamentos de analítica II\n",
    "\n",
    "*PROYECTO DE PRONOSTICO DE PSA PARA PACIENTES ENTRE 48 Y 60 AÑOS*\n",
    "\n",
    "GRUPO:\n",
    "- Esteban Ordoñez\n",
    "- Raul Echeverry\n",
    "- Fabian Salazar Figueroa\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Descripción**\n",
    "\n",
    "Contexto de negocio.\n",
    "\n",
    "El cáncer de próstata es uno de los tipos de cáncer más comunes en hombres. La detección temprana es crucial para mejorar las tasas de supervivencia. La prueba de antígeno prostático específico (PSA) puede ayudar a detectar el cáncer de próstata en etapas tempranas, cuando es más tratable. Sin embargo, el PSA no es específico para el cáncer de próstata y puede estar elevado en otras condiciones como prostatitis o hiperplasia prostática benigna (HPB).\n",
    "\n",
    "El antígeno prostático específico (PSA) es una proteína producida por células normales y malignas de la glándula prostática. La prueba del PSA mide el nivel de esta proteína en la sangre y es uno de los métodos más utilizados para el tamizaje del cáncer de próstata.\n",
    "\n",
    "La EPS SaludPorTi, está interesado en priorizar la toma de está prueba, aumentando la demanda y detención temprana del Cáncer de Próstata.\n",
    "\n",
    "Problema de negocio\n",
    "La empresa ha decidido contratarlos para que construyan un modelo predictivo que permita estimar la probabilidad de que un usuario entre 48 y 60 años de edad presente resultados anormales de PSA.\n",
    "\n",
    "\n",
    "Contexto analítico\n",
    "\n",
    "Se espera que entrene diferentes familias de modelos predictivos de clasificación (SVC con diferentes kernels, Redes Neuronales poco profundas), precedidos por diferentes procesos de transformación (normalizaciones, imputación, feature engineering, dummificación, PCA, selección de features).\n",
    "\n",
    "\n",
    "La evaluación de la calidad de los flujos de modelos predictivos se debe estimar utilizando la métrica de ROC_AUC.\n",
    "\n",
    "Expliquen sus ideas, el por qué realizan las acciones, y comenten los resultados obtenidos; se espera mucho más que unos bloques de código.\n",
    "La toma de decisiones sobre los datos se debe hacer considerando el contexto del problema y de los datos, no se puede ver todo solamente desde los ojos de los datos, sino también considerar el negocio.\n",
    "Un Científico de Datos debe poder comunicar los puntos importantes de su trabajo en un lenguaje universal para todos los públicos.\n",
    "Todo esto se considerará en la nota."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cita**\n",
    "\n",
    "@misc{fa-ii-2024-ii-flujos-de-modelos-tradicionales,\n",
    "    author = {Daniel Osorio, JavierDiaz},\n",
    "    title = {FA II 2024-II: flujos de modelos tradicionales},\n",
    "    publisher = {Kaggle},\n",
    "    year = {2024},\n",
    "    url = {https://kaggle.com/competitions/fa-ii-2024-ii-flujos-de-modelos-tradicionales}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________________________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Flujo de trabajo completo y detallado**\n",
    "**1.\tDefinición del problema y objetivos del negocio**\n",
    "- Definir el objetivo clave: predecir si un paciente entre 48 y 60 años tendrá un resultado anormal de PSA, ayudando a priorizar las pruebas para una detección temprana del cáncer de próstata.\n",
    "\n",
    "**2.\tAnálisis exploratorio de datos (EDA)**\n",
    "- Revisar la distribución de las variables.\n",
    "- Verificar la correlación entre las características.\n",
    "- Detectar valores faltantes, outliers, y estudiar las relaciones de las características con la variable objetivo.\n",
    "- Explorar diferencias entre las clases objetivo (PSA normal vs anormal).\n",
    "\n",
    "**3.\tPreprocesamiento de datos**\n",
    "- Analisis de importancia de variables\n",
    "- Formateo de variables.\n",
    "- Manejo de outliers.\n",
    "- Imputar valores faltantes.\n",
    "- Escalar las variables numéricas (normalización o estandarización).\n",
    "- Codificar las variables categóricas mediante dummificación (One-Hot Encoding).\n",
    "- Considerar técnicas como reducción de dimensionalidad (PCA) si es necesario.\n",
    "- Crear un pipeline de preprocesamiento para que los datos estén listos para usarse en los modelos.\n",
    "\n",
    "Abordaremos esto de una forma donde haremos un preprocesamiento previo a algunas variables que consideramos no alterará ni pondrá en riesgo nuestro dataframe para el conocido **data leakage**, por eso algunos pasos se verán reflejados en el entrenamiento.\n",
    "\n",
    "**4.\tSelección de modelos iniciales**\n",
    "- Probar diferentes familias de modelos de clasificación:\n",
    "    - **Support Vector Classifier (SVC)** con diferentes kernels (linear, rbf, poly).\n",
    "    - Otros modelos iniciales como **Redes Neuronales** poco profundas (que implementaremos más adelante).\n",
    "- Comparar su rendimiento inicial usando métricas como **ROC-AUC**.\n",
    "    \n",
    "**5.\tOptimización de hiperparámetros (Optimización Bayesiana)**\n",
    "- Optimizar los hiperparámetros del modelo, como C, gamma para SVC y n_components para PCA usando optimización bayesiana.\n",
    "- Ajustar los modelos para maximizar su rendimiento, evaluando en cada iteración con **validación cruzada**.\n",
    "\n",
    "**6.\tEvaluación de los modelos optimizados**\n",
    "- 6.1 Visualización de las curvas ROC para los modelos optimizados:\n",
    "    - Aquí incluimos la visualización de las curvas ROC para los modelos optimizados (SVC con PCA) y comparamos su rendimiento.\n",
    "    - Calcular el área bajo la curva (AUC) para comparar el rendimiento de los diferentes modelos en términos de sensibilidad y especificidad.\n",
    "- 6.2 Comparación con otros modelos adicionales (redes neuronales poco profundas):\n",
    "    - Añadir un modelo de redes neuronales poco profundas (MLPClassifier) y comparar su curva ROC con los otros modelos.\n",
    "    - Realizar una comparación directa en términos de **ROC-AUC**.\n",
    "\n",
    "**7.\tInterpretación de resultados y recomendaciones**\n",
    "- Interpretar los resultados de los modelos, incluyendo las métricas clave como ROC-AUC y la importancia de las características (en el caso del modelo SVC, a través de los vectores soporte o coeficientes).\n",
    "- Seleccionar el mejor modelo en base a las métricas y explicar cómo este modelo puede ser implementado en la práctica para priorizar las pruebas de PSA.\n",
    "\n",
    "**8.\tConclusiones y comunicación**\n",
    "- Resumir los hallazgos clave para diferentes audiencias (tanto técnicas como no técnicas).\n",
    "- Presentar recomendaciones basadas en el análisis y los resultados de los modelos para la toma de decisiones empresariales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Definición del problema y objetivos del negocio**\n",
    "\n",
    "**Objetivo:** Desarrollar un modelo de clasificación que prediga si un paciente entre 48 y 60 años tendrá un nivel anormal de PSA. Esto es clave para priorizar las pruebas y detectar el cáncer de próstata en etapas tempranas.\n",
    "\n",
    "- **Predicción:** Etiqueta binaria, con resultados normales (0) o anormales (1) en la prueba de PSA.\n",
    "- **Métrica de evaluación:** ROC-AUC, dado que queremos balancear entre los falsos positivos y falsos negativos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Librerías a básicas utilizar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manejo de análisis de datos a través de dataframes (datos tabulares)\n",
    "import pandas as pd\n",
    "# Manipulación de arreglos y análisis numérico\n",
    "import numpy as np\n",
    "# Visualización de datos con gráficos estadísticos\n",
    "import seaborn as sns\n",
    "# Visualización de datos con gráficos generales\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Librería para el manejo de expresiones regulares\n",
    "import re\n",
    "# Librería para identificar variables categóricas y numéricas, y calcular asociaciones\n",
    "from dython.nominal import identify_nominal_columns, identify_numeric_columns, correlation_ratio, associations\n",
    "# Librería matemática estándar de Python\n",
    "import math\n",
    "# Clasificación de bosque aleatorio\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Base para crear estimadores personalizados en scikit-learn\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "# Herramientas de scikit-learn para la separación de datos y validación cruzada\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "# Escalador estándar para normalizar las características\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "# Imputador para manejar valores faltantes\n",
    "from sklearn.impute import SimpleImputer\n",
    "# Clasificador de Máquinas de Soporte Vectorial (SVM)\n",
    "from sklearn.svm import SVC\n",
    "# Clasificador de redes neuronales multicapa\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# Métricas de evaluación para modelos de clasificación\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "# Creación de pipelines para el procesamiento y modelado de datos\n",
    "from sklearn.pipeline import Pipeline\n",
    "# Para realizar transformaciones en columnas específicas del DataFrame\n",
    "from sklearn.compose import ColumnTransformer\n",
    "# Reducción de la dimensionalidad con análisis de componentes principales (PCA)\n",
    "from sklearn.decomposition import PCA\n",
    "# Selección de características basadas en la información mutua\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "# Optimización bayesiana para la búsqueda de hiperparámetros\n",
    "from bayes_opt import BayesianOptimization\n",
    "# Librerías estadísticas\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency\n",
    "# Winsorización de datos para limitar los valores extremos\n",
    "from scipy.stats.mstats import winsorize\n",
    "# Mostrar imágenes en el notebook\n",
    "from IPython.display import Image\n",
    "# Desactivar warnings innecesarios\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Lectura de datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_parquet(r\"https://github.com/alfa7g7/Fundamentos-analitica-II/raw/refs/heads/main/UNIDAD%20II/Proyecto%20final%20PSA/Data/df_train.parquet\")\n",
    "print(df_train.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_parquet(r\"https://github.com/alfa7g7/Fundamentos-analitica-II/raw/refs/heads/main/UNIDAD%20II/Proyecto%20final%20PSA/Data/df_test.parquet\")\n",
    "print(df_test.shape)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pasar a csv para análisis campesino\n",
    "#df.to_csv(r\"C:\\Users\\alfa7\\OneDrive\\Documentos\\ICESI\\MAESTRIA CIENCIA DE DATOS\\2do semestre\\Fundamentos de analitica II\\Unidad II\\Proyecto PSA\\Data\\df_train.csv\")\n",
    "#df1.to_csv(r\"C:\\Users\\alfa7\\OneDrive\\Documentos\\ICESI\\MAESTRIA CIENCIA DE DATOS\\2do semestre\\Fundamentos de analitica II\\Unidad II\\Proyecto PSA\\Data\\df_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Análisis Explotarorio de Datos**\n",
    "\n",
    "El análisis exploratorio nos ayudará a comprender mejor el conjunto de datos, incluyendo:\n",
    "\n",
    "- Distribución de la variable objetivo (PSA normal vs anormal).\n",
    "- Características demográficas (edad, estado de salud general, historial médico).\n",
    "- Visualización de correlaciones entre las características y el resultado de PSA.\n",
    "- Outliers y valores faltantes en las variables predictoras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisamos las dimensiones del conjunto de datos  \n",
    "rows, col = df_train.shape\n",
    "print (\"Dimensiones del conjunto de datos: {}\" . format (df_train.shape))\n",
    "print ('Filas:', rows, '\\nColumnas:', col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Distribución de la variable objetivo (PSA normal vs anormal).\n",
    "\n",
    "    Se denota que un valor 0 en la variable significa un estado normal y un valor 1 se refiere a un valor anormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos valores únicos\n",
    "print(df_train['Target'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos si tiene valores faltantes\n",
    "missing_values = df_train['Target'].isnull().sum()\n",
    "print(f\"Valores faltantes en 'Target': {missing_values}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_target_distribution(df, target_column='Target', target_labels={0: 'Normal', 1: 'Anormal'}, \n",
    "                             palette='viridis', colors=['#66b3ff','#ff9999'], \n",
    "                             bar_title='Distribución de PSA Normal y Anormal', \n",
    "                             pie_title='Porcentaje de PSA Normal vs Anormal', \n",
    "                             xlabel='Estado del PSA', ylabel='Cantidad de Observaciones'):\n",
    "    \"\"\"\n",
    "    Genera el conteo y los gráficos de barras y circular para la variable objetivo.\n",
    "\n",
    "    Parámetros:\n",
    "    - df: DataFrame que contiene los datos.\n",
    "    - target_column: Nombre de la columna objetivo en el DataFrame.\n",
    "    - target_labels: Diccionario para mapear los valores de la columna objetivo a etiquetas.\n",
    "    - palette: Paleta de colores para el gráfico de barras.\n",
    "    - colors: Lista de colores para el gráfico circular.\n",
    "    - bar_title: Título del gráfico de barras.\n",
    "    - pie_title: Título del gráfico circular.\n",
    "    - xlabel: Etiqueta del eje X en el gráfico de barras.\n",
    "    - ylabel: Etiqueta del eje Y en el gráfico de barras.\n",
    "\n",
    "    Retorna:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Contar las observaciones en cada categoría\n",
    "    conteo = df[target_column].map(target_labels).value_counts()\n",
    "    print(\"Conteo de observaciones en cada categoría:\")\n",
    "    print(conteo)\n",
    "    print(\"\\nPorcentajes de cada categoría:\")\n",
    "    print(conteo / conteo.sum() * 100)\n",
    "\n",
    "    # Gráfico de barras de Target\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.countplot(x=df[target_column].map(target_labels), palette=palette)\n",
    "    plt.title(bar_title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()\n",
    "\n",
    "    # Gráfico circular de Target\n",
    "    plt.figure(figsize=(6,6))\n",
    "    conteo.plot.pie(autopct='%1.1f%%', startangle=90, colors=colors, labels=conteo.index)\n",
    "    plt.title(pie_title)\n",
    "    plt.ylabel('')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llamamos a la funcion con nuestro dataframe df_train\n",
    "plot_target_distribution(df_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encontramos que la base de datos no está aceptablemente balanceada, con un 28.5% de los individuos presentando una un estado anormal de PSA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Características adicionales y demográficas (medicamentos, imc, estado de salud general, historial médico)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Información general del dataset\n",
    "print(df_train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descripción estadística de variables numéricas\n",
    "#print(df_train.describe())\n",
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descripción estadística de variables categóricas\n",
    "#print(df_train.describe(include=['object', 'category']))\n",
    "df_train.describe(include=['object', 'category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener los tipos de datos\n",
    "print(df_train.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Análisis Univariado\n",
    "\n",
    "    Antes de explorar las relaciones entre variables, es útil analizar cada variable individualmente.\n",
    "\n",
    "    - Variables Numéricas\n",
    "        Para cada variable numérica:\n",
    "        - Histograma: Para ver la distribución.\n",
    "        - Boxplot: Para identificar outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_numerical_variables(df, numerical_vars=None, figsize=(20, 4)):\n",
    "    \"\"\"\n",
    "    Genera histogramas y boxplots para variables numéricas en un DataFrame.\n",
    "\n",
    "    Parámetros:\n",
    "    - df: DataFrame que contiene los datos.\n",
    "    - numerical_vars: Lista de variables numéricas a graficar. Si es None, se seleccionan automáticamente.\n",
    "    - figsize: Tamaño de la figura para cada variable (ancho, alto).\n",
    "\n",
    "    Retorna:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    if numerical_vars is None:\n",
    "        numerical_vars = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    \n",
    "    for var in numerical_vars:\n",
    "        plt.figure(figsize=figsize)\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.histplot(df[var].dropna(), kde=True)\n",
    "        plt.title(f'Distribución de {var}')\n",
    "        plt.xlabel(var)\n",
    "        plt.ylabel('Frecuencia')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        sns.boxplot(x=df[var])\n",
    "        plt.title(f'Boxplot de {var}')\n",
    "        plt.xlabel(var)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llamamos a la función con nuestro dataframe df_train\n",
    "plot_numerical_variables(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Análisis Univariado\n",
    "\n",
    "    Antes de explorar las relaciones entre variables, es útil analizar cada variable individualmente.\n",
    "    - Variables Categóricas\n",
    "        Para cada variable categórica:\n",
    "        - Conteo de categorías: Usando gráficos de barras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_categorical_variables(df, n_cols=3, rotation=45, figsize_multiplier=(6, 5)):\n",
    "    \"\"\"\n",
    "    Genera gráficos de barras para variables categóricas en un DataFrame,\n",
    "    incluyendo etiquetas de frecuencia en cada barra.\n",
    "\n",
    "    Parámetros:\n",
    "    - df: DataFrame que contiene los datos.\n",
    "    - n_cols: Número de gráficos por fila (columnas en la cuadrícula).\n",
    "    - rotation: Grados de rotación de las etiquetas del eje X.\n",
    "    - figsize_multiplier: Tupla que multiplica el tamaño de la figura (ancho, alto) por (n_cols, n_rows).\n",
    "\n",
    "    Retorna:\n",
    "    - None\n",
    "    \"\"\"\n",
    "\n",
    "    # Seleccionamos las variables categóricas\n",
    "    categorical_vars = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    n_vars = len(categorical_vars)\n",
    "\n",
    "    if n_vars == 0:\n",
    "        print(\"No hay variables categóricas en el DataFrame.\")\n",
    "        return\n",
    "\n",
    "    # Calculamos el número de filas necesario\n",
    "    n_rows = math.ceil(n_vars / n_cols)\n",
    "\n",
    "    # Creamos la figura y los ejes\n",
    "    figsize = (n_cols * figsize_multiplier[0], n_rows * figsize_multiplier[1])\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Iteramos sobre las variables categóricas y los ejes\n",
    "    for idx, var in enumerate(categorical_vars):\n",
    "        sns.countplot(x=var, data=df, ax=axes[idx])\n",
    "        axes[idx].set_title(f'Distribución de {var}')\n",
    "        axes[idx].tick_params(axis='x', rotation=rotation)\n",
    "        axes[idx].set_xlabel(var)\n",
    "        axes[idx].set_ylabel('Frecuencia')\n",
    "\n",
    "        # Agregar etiquetas de frecuencia encima de las barras\n",
    "        total = len(df)\n",
    "        for p in axes[idx].patches:\n",
    "            height = p.get_height()\n",
    "            axes[idx].annotate(f'{int(height)}', \n",
    "                               (p.get_x() + p.get_width() / 2, height + 0.001 * total),\n",
    "                               ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    # Eliminar los subplots vacíos si los hay\n",
    "    for i in range(idx + 1, n_rows * n_cols):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llamamos a la función con nuestro dataframe df_train\n",
    "plot_categorical_variables(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Análisis Bivariado\n",
    "\n",
    "    Ahora, exploraremos cómo cada variable se relaciona con la variable objetivo 'Target'.\n",
    "\n",
    "    - Variables Numéricas vs. Target\n",
    "        - Boxplots por categoría de Target: Para comparar distribuciones.\n",
    "        - Histogramas separados: Para ver distribuciones por clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_numerical_vs_target(df, target_var='Target', numerical_vars=None, figsize=(20, 4)):\n",
    "    \"\"\"\n",
    "    Genera boxplots y gráficos de densidad (KDE) para variables numéricas en función de la variable objetivo.\n",
    "\n",
    "    Parámetros:\n",
    "    - df: DataFrame que contiene los datos.\n",
    "    - target_var: Nombre de la variable objetivo en el DataFrame.\n",
    "    - numerical_vars: Lista de variables numéricas a graficar. Si es None, se seleccionan automáticamente.\n",
    "    - figsize: Tamaño de la figura para cada variable (ancho, alto).\n",
    "\n",
    "    Retorna:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Si no se proporcionan numerical_vars, las seleccionamos del DataFrame\n",
    "    if numerical_vars is None:\n",
    "        numerical_vars = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "        # Excluir la variable objetivo si es numérica\n",
    "        if target_var in numerical_vars:\n",
    "            numerical_vars.remove(target_var)\n",
    "\n",
    "    # Verificar si hay variables numéricas\n",
    "    if not numerical_vars:\n",
    "        print(\"No se encontraron variables numéricas en el DataFrame.\")\n",
    "        return\n",
    "\n",
    "    for var in numerical_vars:\n",
    "        plt.figure(figsize=figsize)\n",
    "        \n",
    "        # Boxplot de la variable vs Target\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.boxplot(x=target_var, y=var, data=df)\n",
    "        plt.title(f'{var} vs {target_var}')\n",
    "        plt.xlabel(target_var)\n",
    "        plt.ylabel(var)\n",
    "        \n",
    "        # Gráfico de densidad (KDE) de la variable por Target\n",
    "        plt.subplot(1, 2, 2)\n",
    "        sns.kdeplot(data=df, x=var, hue=target_var, fill=True)\n",
    "        plt.title(f'Distribución de {var} por {target_var}')\n",
    "        plt.xlabel(var)\n",
    "        plt.ylabel('Densidad')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llamar a la función con nuestro dataframe df_train\n",
    "plot_numerical_vs_target(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Análisis Bivariado\n",
    "\n",
    "    Ahora, exploraremos cómo cada variable se relaciona con la variable objetivo 'Target'.\n",
    "    - Variables Categóricas vs. Target\n",
    "        - Tablas de contingencia y gráficos de barras apilados: Para ver la distribución de 'Target' dentro de cada categoría."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_categorical_vs_target(df, target_var='Target', n_cols=3, rotation=45, figsize_multiplier=(5, 4)):\n",
    "    \"\"\"\n",
    "    Genera gráficos de conteo para variables categóricas en función de la variable objetivo.\n",
    "\n",
    "    Parámetros:\n",
    "    - df: DataFrame que contiene los datos.\n",
    "    - target_var: Nombre de la variable objetivo en el DataFrame.\n",
    "    - n_cols: Número de gráficos por fila (columnas en la cuadrícula).\n",
    "    - rotation: Grados de rotación de las etiquetas del eje X.\n",
    "    - figsize_multiplier: Tupla que multiplica el tamaño de la figura (ancho, alto) por (n_cols, n_rows).\n",
    "\n",
    "    Retorna:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Seleccionamos las variables categóricas\n",
    "    categorical_vars = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    n_vars = len(categorical_vars)\n",
    "\n",
    "    if n_vars == 0:\n",
    "        print(\"No hay variables categóricas en el DataFrame.\")\n",
    "        return\n",
    "\n",
    "    # Definimos el número de filas necesario\n",
    "    n_rows = math.ceil(n_vars / n_cols)\n",
    "\n",
    "    # Creamos la figura y los ejes\n",
    "    figsize = (n_cols * figsize_multiplier[0], n_rows * figsize_multiplier[1])\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Iteramos sobre las variables categóricas y los ejes\n",
    "    for idx, var in enumerate(categorical_vars):\n",
    "        sns.countplot(x=var, hue=target_var, data=df, ax=axes[idx])\n",
    "        axes[idx].set_title(f'Distribución de {var} por {target_var}')\n",
    "        axes[idx].tick_params(axis='x', rotation=rotation)\n",
    "        axes[idx].set_xlabel(var)\n",
    "        axes[idx].set_ylabel('Frecuencia')\n",
    "        for p in axes[idx].patches:\n",
    "            height = p.get_height()\n",
    "            axes[idx].annotate(f'{int(height)}', (p.get_x() + p.get_width() / 2, height + 0.03),\n",
    "                       ha='center', va='bottom', fontsize=7)\n",
    "\n",
    "\n",
    "    # Eliminar los subplots vacíos si los hay\n",
    "    for i in range(idx + 1, n_rows * n_cols):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llamamos a la funcion con nuestro dataframe df_train\n",
    "plot_categorical_vs_target(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_categorical_distributions(df, target_var, n_cols=3, normal_color='#66b3ff', anormal_color='#ff9999', figsize_multiplier=(6, 5)):\n",
    "    \"\"\"\n",
    "    Genera gráficos de barras apiladas para cada variable categórica en un DataFrame, mostrando la proporción\n",
    "    de las categorías de una variable objetivo dentro de cada variable categórica.\n",
    "\n",
    "    Parámetros:\n",
    "    - df: DataFrame que contiene las variables categóricas y la variable objetivo.\n",
    "    - target_var: Nombre de la variable objetivo (debe ser binaria o categórica).\n",
    "    - n_cols: Número de gráficos por fila (predeterminado: 3).\n",
    "    - normal_color: Color de las barras de la categoría 'Normal' (predeterminado: '#66b3ff').\n",
    "    - anormal_color: Color de las barras de la categoría 'Anormal' (predeterminado: '#ff9999').\n",
    "    - figsize_multiplier: Multiplicador para el tamaño de la figura (predeterminado: (6, 5)).\n",
    "\n",
    "    Retorna:\n",
    "    - Muestra los gráficos de distribución apilados.\n",
    "    \"\"\"\n",
    "\n",
    "    # Seleccionar variables categóricas\n",
    "    categorical_vars = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    n_vars = len(categorical_vars)\n",
    "    \n",
    "    if n_vars == 0:\n",
    "        print(\"No hay variables categóricas en el DataFrame.\")\n",
    "        return\n",
    "\n",
    "\n",
    "    # Calcular el número de filas necesario\n",
    "    n_rows = math.ceil(n_vars / n_cols)\n",
    "\n",
    "    # Crear la figura y los ejes\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * figsize_multiplier[0], n_rows * figsize_multiplier[1]))\n",
    "    axes = axes.flatten()  # Aplanamos la matriz de ejes para facilitar la iteración\n",
    "\n",
    "    # Iterar sobre las variables categóricas y los ejes para generar los gráficos\n",
    "    for idx, var in enumerate(categorical_vars):\n",
    "        cross_tab = pd.crosstab(df[var], df[target_var], normalize='index')\n",
    "        cross_tab = cross_tab.reset_index()\n",
    "\n",
    "        # Asegurarnos de que las columnas están ordenadas correctamente\n",
    "        cross_tab = cross_tab.rename(columns={0: 'Normal', 1: 'Anormal'})\n",
    "\n",
    "        # Crear el gráfico de barras apiladas\n",
    "        bars_normal = axes[idx].bar(x=cross_tab[var], height=cross_tab['Normal'], label='Normal', color=normal_color)\n",
    "        bars_anormal = axes[idx].bar(x=cross_tab[var], height=cross_tab['Anormal'], bottom=cross_tab['Normal'], label='Anormal', color=anormal_color)\n",
    "\n",
    "        # Agregar etiquetas de porcentaje\n",
    "        for i in range(len(cross_tab)):\n",
    "            normal_height = cross_tab['Normal'].iloc[i]\n",
    "            anormal_height = cross_tab['Anormal'].iloc[i]\n",
    "            total_height = normal_height + anormal_height\n",
    "\n",
    "            # Etiqueta para la categoría 'Normal'\n",
    "            if normal_height > 0:\n",
    "                axes[idx].text(i, normal_height / 2, f'{normal_height * 100:.1f}%', ha='center', va='center', color='white', fontsize=10)\n",
    "            # Etiqueta para la categoría 'Anormal'\n",
    "            if anormal_height > 0:\n",
    "                axes[idx].text(i, normal_height + anormal_height / 2, f'{anormal_height * 100:.1f}%', ha='center', va='center', color='white', fontsize=10)\n",
    "\n",
    "        axes[idx].set_title(f'Distribución de {target_var} dentro de {var}')\n",
    "        axes[idx].set_ylabel('Proporción')\n",
    "        axes[idx].set_xlabel(var)\n",
    "        axes[idx].legend(title=target_var, labels=['Normal', 'Anormal'])\n",
    "        axes[idx].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    # Eliminar los subplots vacíos si los hay\n",
    "    for i in range(idx + 1, n_rows * n_cols):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llamamos a la funcion con nuestro dataframe df_train\n",
    "plot_categorical_distributions(df_train, 'Target', n_cols=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observamos valores únicos de las variables\n",
    "unique = df_train.nunique()\n",
    "plt.figure(figsize=(35, 6))\n",
    "unique.plot(kind='bar', color='blue', hatch='//')\n",
    "plt.title('Elementos únicos en cada columna')\n",
    "plt.ylabel('Conteo')\n",
    "for i, v in enumerate(unique.values):\n",
    "    plt.text(i, v+1, str(v), color='black', fontweight='bold', ha='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Visualización de correlaciones entre las características y el resultado de PSA.\n",
    "\n",
    "    Vamos a utilizar la matriz de correlación para corroborar los indicios sobre las relaciones presentes entre algunas de las características y la variable objetivo.\n",
    "    - Al contar con variables categóricas (la dependiente y varias dependientes) no podemos utilizar la matriz de correlación clásica que utiliza el coeficiente de correlación de Pearson ya que este tan sólo nos sirve para relación entre variables continuas.\n",
    "    - Al haber varias características categóricas utilizamos la función associations de dython que utiliza el coeficiente de Pearson para continuas vs continuas, la razón de correlación para continuas vs categóricas y Cramer's V o Theil's U para categóricas vs categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features=identify_nominal_columns(df_train)\n",
    "continuous_features=identify_numeric_columns(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_correlation= associations(df_train, filename= r'C:\\Users\\alfa7\\OneDrive\\Documentos\\ICESI\\MAESTRIA CIENCIA DE DATOS\\2do semestre\\Fundamentos de analitica II\\Unidad II\\Proyecto PSA\\Images\\Complete_correlation.png', figsize=(30,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_correlation= associations(df_train, display_rows=['Target'], filename= r'C:\\Users\\alfa7\\OneDrive\\Documentos\\ICESI\\MAESTRIA CIENCIA DE DATOS\\2do semestre\\Fundamentos de analitica II\\Unidad II\\Proyecto PSA\\Images\\Target_correlation.png', figsize=(30,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detallamos primero las variables independientes que más influencian a la objetivo (aunque ninguna tiene una alta correlación >= 0.7 o <= -0.7) en orden descendente es decir de mayor a menor importancia:\n",
    "- EDAD 0.18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos la matriz de correlación entre las variables continuas\n",
    "selected_column= df_train[continuous_features]\n",
    "continuous_df = selected_column.copy()\n",
    "\n",
    "continuous_correlation= associations(continuous_df, filename= r'C:\\Users\\alfa7\\OneDrive\\Documentos\\ICESI\\MAESTRIA CIENCIA DE DATOS\\2do semestre\\Fundamentos de analitica II\\Unidad II\\Proyecto PSA\\Images\\Continuous_correlation.png', figsize=(20,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detallamos primero las variables independientes que más influencian a la objetivo (aunque ninguna tiene una alta correlación >= 0.7 o <= -0.7) en orden descendente es decir de mayor a menor importancia:\n",
    "\n",
    "Revisamos aquellas variables independientes continuas que pueden llegar a generar problemas al encontrarse altamente correlacionadas entre sí (problemas de multicolinealidad, redundancia, complejitud innecesaria del modelo):\n",
    "- **psa_max_gr_flia** y **psa_min_gr_flia** *0.78*\n",
    "- **Pendiente** e **Intercepto** *-0.78*\n",
    "- **CANTIDAD_SERVICIOS** y **conteo_dx_diferentes** *0.77*\n",
    "- **min_Tiempo_CP_Fliar** y **Cant_Fliar_CP** *0.76*\n",
    "- **Pendiente_flia** y **Pendiente** *0.75*\n",
    "- **Pendiente_flia** y **Intercepto_flia** *-0.75*\n",
    "- **Cant_riesgos_flia_mean** y **Cant_Fliar_riesgos** *0.73*\n",
    "- **Intercepto_flia** y **Intercepto** *0.73*\n",
    "- **Promedio_costo_flia** y **Promedio_costo** *0.71*\n",
    "- **Cant_riesgos_flia_mean** y **RIESGOS** *0.69*\n",
    "- **CANTIDAD_SERVICIOS** y **MEDICINA_ESPECIALIZADA** *0.69*\n",
    "- **Cant_Fliar_riesgos** y **RIESGOS** *0.63*\n",
    "\n",
    "Estas relaciones nos ayudarán a decidir el tema de eliminación de variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos la matriz de correlación entre las variables categoricas\n",
    "selected_column= df_train[categorical_features]\n",
    "categorical_df = selected_column.copy()\n",
    "\n",
    "categorical_correlation= associations(categorical_df, filename= r'C:\\Users\\alfa7\\OneDrive\\Documentos\\ICESI\\MAESTRIA CIENCIA DE DATOS\\2do semestre\\Fundamentos de analitica II\\Unidad II\\Proyecto PSA\\Images\\Categorical_correlation.png', figsize=(20,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detallamos primero las variables independientes que más influencian a la objetivo (aunque ninguna tiene una alta correlación >= 0.7 o <= -0.7) en orden descendente es decir de mayor a menor importancia:\n",
    "\n",
    "Revisamos aquellas variables independientes continuas que pueden llegar a generar problemas al encontrarse altamente correlacionadas entre sí (problemas de multicolinealidad, redundancia, complejitud innecesaria del modelo):\n",
    "\n",
    "- **CEREBRAL** y **ENFERMEDAD_RENAL** *0.92*\n",
    "- **CORONARIOS** y **ENFERMEDAD_RENAL** *0.86*\n",
    "- **HIPERTENSION** y **HIPERTENSION_FAMILIAR** *0.86*\n",
    "- **CORONARIOS** y **CEREBRAL** *0.85*\n",
    "- **DIABETES** y **DIABETES_FAMILIAR** *0.84*\n",
    "- **CANCER_OTRO_SITIO** y **CANCER_OTRO_SITIO_FAMILIAR** *0.83*\n",
    "- **CANCER_OTRO_SITIO** y **ENFERMEDAD_RENAL** *0.80*\n",
    "- **CORONARIOS** y **CORONARIOS_FAMILIAR** *0.80*\n",
    "- **DIABETES** y **ENFERMEDAD_RENAL** *0.80*\n",
    "- **CANCER_OTRO_SITIO** y **CEREBRAL** *0.79*\n",
    "- **CEREBRAL** y **OTROS_ANTECEDENTES_VASCULARES** *0.79*\n",
    "- **DIABETES** y **CEREBRAL** *0.79*\n",
    "- **ENFERMEDAD_RENAL** y **OTROS_ANTECEDENTES_VASCULARES** *0.79*\n",
    "- **CANCER_OTRO_SITIO** y **CORONARIOS** *0.77*\n",
    "- **DIABETES** y **CORONARIOS** *0.77*\n",
    "- **HIPERTENSION** y **ENFERMEDAD_RENAL** *0.77*\n",
    "- **HIPERTENSION** y **CEREBRAL** *0.77*\n",
    "- **CORONARIOS** y **OTROS_ANTECEDENTES_VASCULARES** *0.76*\n",
    "- **HIPERTENSION** y **DIABETES** *0.76*\n",
    "- **HIPERTENSION** y **CORONARIOS** *0.76*\n",
    "- **CANCER_OTRO_SITIO** y **HIPERTENSION** *0.75*\n",
    "- **CANCER_OTRO_SITIO** y **DIABETES** *0.75*\n",
    "- **CANCER_OTRO_SITIO** y **OTROS_ANTECEDENTES_VASCULARES** *0.74*\n",
    "- **CEREBRAL** y **CEREBRAL_FAMILIAR** *0.74*\n",
    "- **DIABETES** y **OTROS_ANTECEDENTES_VASCULARES** *0.74*\n",
    "- **ENFERMEDAD_RENAL** y **ENFERMEDAD_RENAL_FAMILIAR** *0.73*\n",
    "- **HIPERTENSION** y **OTROS_ANTECEDENTES_VASCULARES** *0.73*\n",
    "- **ESTADO_CIVI** y **PROGRAMA** *0.73*\n",
    "- **CANCER_MAMA_FAMILIAR** y **CANCER_OTRO_SITIO** *0.71*\n",
    "- **CANCER_MAMA_FAMILIAR** y **CANCER_OTRO_SITIO_FAMILIAR** *0.71*\n",
    "- **CANCER_MAMA_FAMILIAR** y **HIPERTENSION** *0.71*\n",
    "- **CANCER_MAMA_FAMILIAR** y **HIPERTENSION_FAMILIAR** *0.71*\n",
    "- **CANCER_MAMA_FAMILIAR** y **DIABETES** *0.71*\n",
    "- **CANCER_MAMA_FAMILIAR** y **DIABETES_FAMILIAR** *0.71*\n",
    "- **CANCER_MAMA_FAMILIAR** y **CORONARIOS** *0.71*\n",
    "- **CANCER_MAMA_FAMILIAR** y **CORONARIOS_FAMILIAR** *0.71*\n",
    "- **CANCER_MAMA_FAMILIAR** y **CEREBRAL** *0.71*\n",
    "- **CANCER_MAMA_FAMILIAR** y **CEREBRAL_FAMILIAR** *0.71*\n",
    "- **CANCER_MAMA_FAMILIAR** y **ENFERMEDAD_RENAL** *0.71*\n",
    "- **CANCER_MAMA_FAMILIAR** y **ENFERMEDAD_RENAL_FAMILIAR** *0.71*\n",
    "- **CANCER_MAMA_FAMILIAR** y **OTROS_ANTECEDENTES_VASCULARES** *0.71*\n",
    "- **CANCER_OTRO_SITIO** y **HIPERTENSION_FAMILIAR** *0.71*\n",
    "- **CANCER_OTRO_SITIO** y **DIABETES_FAMILIAR** *0.71*\n",
    "- **CANCER_OTRO_SITIO** y **CORONARIOS_FAMILIAR** *0.71*\n",
    "- **CANCER_OTRO_SITIO** y **CEREBRAL_FAMILIAR** *0.71*\n",
    "- **CANCER_OTRO_SITIO** y **ENFERMEDAD_RENAL_FAMILIAR** *0.71*\n",
    "- **CANCER_OTRO_SITIO_FAMILIAR** y **HIPERTENSION** *0.71*\n",
    "- **CANCER_OTRO_SITIO_FAMILIAR** y **HIPERTENSION_FAMILIAR** *0.71*\n",
    "- **CANCER_OTRO_SITIO_FAMILIAR** y **DIABETES** *0.71*\n",
    "- **CANCER_OTRO_SITIO_FAMILIAR** y **DIABETES_FAMILIAR** *0.71*\n",
    "- **CANCER_OTRO_SITIO_FAMILIAR** y **CORONARIOS** *0.71*\n",
    "- **CANCER_OTRO_SITIO_FAMILIAR** y **CORONARIOS_FAMILIAR** *0.71*\n",
    "- **CANCER_OTRO_SITIO_FAMILIAR** y **CEREBRAL** *0.71*\n",
    "- **CANCER_OTRO_SITIO_FAMILIAR** y **CEREBRAL_FAMILIAR** *0.71*\n",
    "- **CANCER_OTRO_SITIO_FAMILIAR** y **ENFERMEDAD_RENAL** *0.71*\n",
    "- **CANCER_OTRO_SITIO_FAMILIAR** y **ENFERMEDAD_RENAL_FAMILIAR** *0.71*\n",
    "- **CANCER_OTRO_SITIO_FAMILIAR** y **OTROS_ANTECEDENTES_VASCULARES** *0.71*\n",
    "- **CEREBRAL** y **ENFERMEDAD_RENAL_FAMILIAR** *0.71*\n",
    "- **CEREBRAL_FAMILIAR** y **ENFERMEDAD_RENAL** *0.71*\n",
    "- **CEREBRAL_FAMILIAR** y **ENFERMEDAD_RENAL_FAMILIAR** *0.71*\n",
    "- **CEREBRAL_FAMILIAR** y **OTROS_ANTECEDENTES_VASCULARES** *0.71*\n",
    "- **CORONARIOS** y **CEREBRAL_FAMILIAR** *0.71*\n",
    "- **CORONARIOS** y **ENFERMEDAD_RENAL_FAMILIAR** *0.71*\n",
    "- **CORONARIOS_FAMILIAR** y **CEREBRAL** *0.71*\n",
    "- **CORONARIOS_FAMILIAR** y **CEREBRAL_FAMILIAR** *0.71*\n",
    "- **CORONARIOS_FAMILIAR** y **ENFERMEDAD_RENAL** *0.71*\n",
    "- **CORONARIOS_FAMILIAR** y **ENFERMEDAD_RENAL_FAMILIAR** *0.71*\n",
    "- **CORONARIOS_FAMILIAR** y **OTROS_ANTECEDENTES_VASCULARES** *0.71*\n",
    "- **DIABETES** y **CORONARIOS_FAMILIAR** *0.71*\n",
    "- **DIABETES** y **CEREBRAL_FAMILIAR** *0.71*\n",
    "- **DIABETES** y **ENFERMEDAD_RENAL_FAMILIAR** *0.71*\n",
    "- **DIABETES_FAMILIAR** y **CORONARIOS** *0.71*\n",
    "- **DIABETES_FAMILIAR** y **CORONARIOS_FAMILIAR** *0.71*\n",
    "- **DIABETES_FAMILIAR** y **CEREBRAL** *0.71*\n",
    "- **DIABETES_FAMILIAR** y **CEREBRAL_FAMILIAR** *0.71*\n",
    "- **DIABETES_FAMILIAR** y **ENFERMEDAD_RENAL** *0.71*\n",
    "- **DIABETES_FAMILIAR** y **ENFERMEDAD_RENAL_FAMILIAR** *0.71*\n",
    "- **DIABETES_FAMILIAR** y **OTROS_ANTECEDENTES_VASCULARES** *0.71*\n",
    "- **ENFERMEDAD_RENAL_FAMILIAR** y **OTROS_ANTECEDENTES_VASCULARES** *0.71*\n",
    "- **HIPERTENSION** y **DIABETES_FAMILIAR** *0.71*\n",
    "- **HIPERTENSION** y **CORONARIOS_FAMILIAR** *0.71*\n",
    "- **HIPERTENSION** y **CEREBRAL_FAMILIAR** *0.71*\n",
    "- **HIPERTENSION** y **ENFERMEDAD_RENAL_FAMILIAR** *0.71*\n",
    "- **HIPERTENSION_FAMILIAR** y **DIABETES** *0.71*\n",
    "- **HIPERTENSION_FAMILIAR** y **DIABETES_FAMILIAR** *0.71*\n",
    "- **HIPERTENSION_FAMILIAR** y **CORONARIOS** *0.71*\n",
    "- **HIPERTENSION_FAMILIAR** y **CORONARIOS_FAMILIAR** *0.71*\n",
    "- **HIPERTENSION_FAMILIAR** y **CEREBRAL** *0.71*\n",
    "- **HIPERTENSION_FAMILIAR** y **CEREBRAL_FAMILIAR** *0.71*\n",
    "- **HIPERTENSION_FAMILIAR** y **ENFERMEDAD_RENAL** *0.71*\n",
    "- **HIPERTENSION_FAMILIAR** y **ENFERMEDAD_RENAL_FAMILIAR** *0.71*\n",
    "- **HIPERTENSION_FAMILIAR** y **OTROS_ANTECEDENTES_VASCULARES** *0.71*\n",
    "- **AGRUPACION_SISTOLICA** y **AGRUPACION_DIASTOLICA** *0.70*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la lista anterior daremos prioridad de observación a estos, debido al alto grado de correlación para nuestras posteriores desiciones \n",
    "\n",
    "- **CEREBRAL** y **ENFERMEDAD_RENAL** *0.92*\n",
    "- **CORONARIOS** y **ENFERMEDAD_RENAL** *0.86*\n",
    "- **HIPERTENSION** y **HIPERTENSION_FAMILIAR** *0.86*\n",
    "- **CORONARIOS** y **CEREBRAL** *0.85*\n",
    "- **DIABETES** y **DIABETES_FAMILIAR** *0.84*\n",
    "- **CANCER_OTRO_SITIO** y **CANCER_OTRO_SITIO_FAMILIAR** *0.83*\n",
    "- **CANCER_OTRO_SITIO** y **ENFERMEDAD_RENAL** *0.80*\n",
    "- **CORONARIOS** y **CORONARIOS_FAMILIAR** *0.80*\n",
    "- **DIABETES** y **ENFERMEDAD_RENAL** *0.80*\n",
    "- **CANCER_OTRO_SITIO** y **CEREBRAL** *0.79*\n",
    "- **CEREBRAL** y **OTROS_ANTECEDENTES_VASCULARES** *0.79*\n",
    "- **DIABETES** y **CEREBRAL** *0.79*\n",
    "- **ENFERMEDAD_RENAL** y **OTROS_ANTECEDENTES_VASCULARES** *0.79*\n",
    "- **CANCER_OTRO_SITIO** y **CORONARIOS** *0.77*\n",
    "- **DIABETES** y **CORONARIOS** *0.77*\n",
    "- **HIPERTENSION** y **ENFERMEDAD_RENAL** *0.77*\n",
    "- **HIPERTENSION** y **CEREBRAL** *0.77*\n",
    "- **CORONARIOS** y **OTROS_ANTECEDENTES_VASCULARES** *0.76*\n",
    "- **HIPERTENSION** y **DIABETES** *0.76*\n",
    "- **HIPERTENSION** y **CORONARIOS** *0.76*\n",
    "- **CANCER_OTRO_SITIO** y **HIPERTENSION** *0.75*\n",
    "- **CANCER_OTRO_SITIO** y **DIABETES** *0.75*\n",
    "- **CANCER_OTRO_SITIO** y **OTROS_ANTECEDENTES_VASCULARES** *0.74*\n",
    "- **CEREBRAL** y **CEREBRAL_FAMILIAR** *0.74*\n",
    "- **DIABETES** y **OTROS_ANTECEDENTES_VASCULARES** *0.74*\n",
    "- **ENFERMEDAD_RENAL** y **ENFERMEDAD_RENAL_FAMILIAR** *0.73*\n",
    "- **HIPERTENSION** y **OTROS_ANTECEDENTES_VASCULARES** *0.73*\n",
    "- **ESTADO_CIVI** y **PROGRAMA** *0.73*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último haremos uso de la correlación tradicional (Coeficiente de correlación de Pearson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos la matriz de correlación\n",
    "corr_matrix = continuous_df.corr()\n",
    "\n",
    "# Convertimos la matriz de correlación en un DataFrame de pares de variables\n",
    "corr_pairs = corr_matrix.unstack()\n",
    "\n",
    "# Convertimos la Serie en un DataFrame y renombramos las columnas\n",
    "corr_pairs = pd.DataFrame(corr_pairs, columns=['Correlación']).reset_index()\n",
    "corr_pairs.columns = ['Variable_1', 'Variable_2', 'Correlación']\n",
    "\n",
    "# Eliminamos los pares duplicados y las autocorrelaciones\n",
    "corr_pairs['Par'] = corr_pairs.apply(lambda x: '-'.join(sorted([x['Variable_1'], x['Variable_2']])), axis=1)\n",
    "corr_pairs = corr_pairs.drop_duplicates(subset='Par')\n",
    "corr_pairs = corr_pairs[corr_pairs['Variable_1'] != corr_pairs['Variable_2']]\n",
    "\n",
    "# Ordenamos las correlaciones de mayor a menor valor absoluto\n",
    "corr_pairs['Correlación_abs'] = corr_pairs['Correlación'].abs()\n",
    "corr_pairs = corr_pairs.sort_values(by='Correlación_abs', ascending=False)\n",
    "\n",
    "# Eliminamos la columna auxiliar 'Correlación_abs' y 'Par'\n",
    "corr_pairs = corr_pairs.drop(columns=['Correlación_abs', 'Par'])\n",
    "\n",
    "# Reiniciamos el índice del DataFrame\n",
    "corr_pairs = corr_pairs.reset_index(drop=True)\n",
    "\n",
    "# Mostramos el resultado\n",
    "print(corr_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observemos las 10 correlaciones más altas\n",
    "top_10_corr = corr_pairs.head(10)\n",
    "print('Top 10 correlaciones más altas:')\n",
    "print(top_10_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pairplot_with_target(df, target_var, diag_kind='kde'):\n",
    "    \"\"\"\n",
    "    Genera un pairplot de las variables numéricas de un DataFrame, destacando las categorías de la variable objetivo.\n",
    "\n",
    "    Parámetros:\n",
    "    - df: DataFrame que contiene las variables numéricas y la variable objetivo.\n",
    "    - target_var: Nombre de la variable objetivo para colorear los gráficos.\n",
    "    - diag_kind: Tipo de gráfico para la diagonal ('kde' o 'hist', por defecto 'kde').\n",
    "\n",
    "    Retorna:\n",
    "    - Un gráfico pairplot con las variables numéricas coloreadas según la variable objetivo.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Seleccionar variables numéricas\n",
    "    numerical_vars = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    num_vars = len(numerical_vars)\n",
    "    \n",
    "    if num_vars == 0:\n",
    "        print(\"No hay variables numéricas en el DataFrame.\")\n",
    "        return\n",
    "\n",
    "    # Asegurarnos de que el target está en la lista\n",
    "    num_vars_with_target = numerical_vars + [target_var]\n",
    "    n_vars = len(num_vars_with_target)\n",
    "    \n",
    "    if n_vars == 0:\n",
    "        print(\"No hay variables numéricas en el DataFrame.\")\n",
    "        return\n",
    "\n",
    "    # Generar el pairplot\n",
    "    sns.pairplot(df[num_vars_with_target], hue=target_var, diag_kind=diag_kind)\n",
    "    \n",
    "    # Mostrar el gráfico\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llamamos a la funcion con nuestro dataframe df_train y con la variable objetivo llamada 'Target'\n",
    "plot_pairplot_with_target(df_train, 'Target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Detectar valores faltantes, outliers, y estudiar las relaciones de las características con la variable objetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cálculo de los z-scores: El z-score mide cuántas desviaciones estándar se aleja un valor de la media de la variable. Se calcula utilizando la fórmula:\n",
    "\n",
    "𝑧 = ( 𝑥 − 𝜇 ) / 𝜎\n",
    "\n",
    "donde:\n",
    "- 𝑥 es el valor individual de la observación.\n",
    "- 𝜇 es la media de todos los valores de la variable.\n",
    "- 𝜎 es la desviación estándar de la variable.\n",
    "\n",
    "Criterio de detección: Se consideran outliers los valores que se encuentran a más de 3 desviaciones estándar de la media (según la Regla Empírica o Regla 68-95-99.7).\n",
    "\n",
    "Identifica los outliers como aquellos valores con un z-score cuyo valor absoluto es mayor que 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Busqueda de outliers con z_scores de scipy\n",
    "def detectar_outliers_zscore(df, threshold=3):\n",
    "    \"\"\"\n",
    "    Detecta outliers en las variables numéricas de un DataFrame utilizando z-scores.\n",
    "\n",
    "    Parámetros:\n",
    "    - df: DataFrame que contiene las variables numéricas.\n",
    "    - threshold: Valor de umbral para los z-scores (por defecto 3).\n",
    "\n",
    "    Retorna:\n",
    "    - Un diccionario donde las claves son los nombres de las variables y los valores son el número de outliers encontrados.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Seleccionar variables numéricas\n",
    "    numerical_vars = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    \n",
    "    # Diccionario para almacenar el número de outliers por variable\n",
    "    outliers_dict = {}\n",
    "\n",
    "    # Iterar sobre las variables numéricas y detectar outliers\n",
    "    for var in numerical_vars:\n",
    "        # Calcular los z-scores, manteniendo el índice original del DataFrame\n",
    "        valid_data = df[var].dropna()  # Eliminar NaN temporalmente\n",
    "        z_scores = stats.zscore(valid_data)\n",
    "\n",
    "        # Filtrar los outliers basados en el umbral de z-score\n",
    "        outliers = df.loc[valid_data.index[np.abs(z_scores) > threshold]]  # Usar los índices del DataFrame original\n",
    "\n",
    "        outliers_dict[var] = outliers.shape[0]\n",
    "        print(f'Número de outliers en {var}: {outliers.shape[0]}')\n",
    "    \n",
    "    return outliers_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llamamaos a la funcion con nuestro dataframe df_train y el valor de umbral threshold =3\n",
    "outliers_info = detectar_outliers_zscore(df_train, threshold=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hagamos otro análisis de outliers; esta vez podemos corroborar lo anterior de manera estadística haciendo uso del método de Tukey (se consideran como datos atípicos aquellos que están 1.5 veces por fuera del rango intercuartil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para calcular los datos atípicos utilizando el método de Tukey\n",
    "def outlier_count(col, data):\n",
    "    print(15*'-' + col + 15*'-')\n",
    "    q75, q25 = np.percentile(data[col], [75, 25])\n",
    "    iqr = q75 - q25\n",
    "    min_val = q25 - (iqr*1.5)\n",
    "    max_val = q75 + (iqr*1.5)\n",
    "    outlier_count = len(np.where((data[col] > max_val) | (data[col] < min_val))[0])\n",
    "    outlier_percent = round(outlier_count/len(data[col])*100, 2)\n",
    "    print('Number of outliers: {}'.format(outlier_count))\n",
    "    print('Percent of data that is outlier: {}%'.format(outlier_percent))\n",
    "    return outlier_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar las columnas de tipo continuas con datos atípicos\n",
    "cont_vars = []\n",
    "for col in list(df_train.select_dtypes('number').columns):\n",
    "    if outlier_count(col, df_train) > 0:\n",
    "        cont_vars.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "wins_dict = {}\n",
    "\n",
    "def test_wins(col, df, wins_dict, lower_limit=0, upper_limit=0, show_plot=True):\n",
    "    wins_data = winsorize(df[col], limits=(lower_limit, upper_limit))\n",
    "    wins_dict[col] = wins_data\n",
    "    if show_plot == True:\n",
    "        plt.figure(figsize=(15,5))\n",
    "        plt.subplot(121)\n",
    "        plt.boxplot(df[col])\n",
    "        plt.title('original {}'.format(col))\n",
    "        plt.subplot(122)\n",
    "        plt.boxplot(wins_data)\n",
    "        plt.title('wins=({},{}) {}'.format(lower_limit, upper_limit, col))\n",
    "        plt.show()\n",
    "    return wins_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificación de la winsorizing\n",
    "wins_dict = {}\n",
    "wins_dict = test_wins(cont_vars[0], df_train, wins_dict, upper_limit=0, show_plot=True) #Sin ajustar\n",
    "wins_dict = test_wins(cont_vars[1], df_train, wins_dict, lower_limit=0, show_plot=True) #Sin ajustar\n",
    "wins_dict = test_wins(cont_vars[2], df_train, wins_dict, lower_limit=0, show_plot=True) #Sin ajustar\n",
    "wins_dict = test_wins(cont_vars[3], df_train, wins_dict, lower_limit=0, show_plot=True) #Sin ajustar\n",
    "wins_dict = test_wins(cont_vars[4], df_train, wins_dict, lower_limit=0, show_plot=True) #Sin ajustar\n",
    "wins_dict = test_wins(cont_vars[5], df_train, wins_dict, upper_limit=0, show_plot=True) #Sin ajustar\n",
    "wins_dict = test_wins(cont_vars[6], df_train, wins_dict, lower_limit=0, show_plot=True) #Sin ajustar\n",
    "wins_dict = test_wins(cont_vars[7], df_train, wins_dict, lower_limit=0, show_plot=True) #Sin ajustar\n",
    "wins_dict = test_wins(cont_vars[8], df_train, wins_dict, lower_limit=0, show_plot=True) #Sin ajustar\n",
    "wins_dict = test_wins(cont_vars[9], df_train, wins_dict, lower_limit=0, show_plot=True) #Sin ajustar\n",
    "wins_dict = test_wins(cont_vars[10], df_train, wins_dict, upper_limit=0, show_plot=True) #Sin ajustar\n",
    "wins_dict = test_wins(cont_vars[11], df_train, wins_dict, lower_limit=0, show_plot=True) #Sin ajustar\n",
    "wins_dict = test_wins(cont_vars[12], df_train, wins_dict, lower_limit=0, show_plot=True) #Sin ajustar\n",
    "wins_dict = test_wins(cont_vars[13], df_train, wins_dict, lower_limit=0, show_plot=True) #Sin ajustar\n",
    "wins_dict = test_wins(cont_vars[14], df_train, wins_dict, lower_limit=0, show_plot=True) #Sin ajustar\n",
    "wins_dict = test_wins(cont_vars[15], df_train, wins_dict, upper_limit=0, show_plot=True) #Sin ajustar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota: Dejaremos propuesto para usarlo de momento no se modifica nada y se hará solo después para revisar que tanto mejora o no con winsorizing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementamos una función que nos permite dibujar nuestros diagramas de cajas y bigotes haciendo especial énfasis en el sesgo de la distribución y los datos atípicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_numerical_features_boxplots(data, columns_list, rows, cols, title):\n",
    "    sns.set_style('darkgrid')\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(30, 14), sharey=True)\n",
    "    fig.suptitle(title, fontsize=25, y=1)\n",
    "    axs = axs.flatten()\n",
    "    outliers_df = pd.DataFrame(columns=['Column', 'Outlier_index', 'Outlier_values'])\n",
    "    for i, col in enumerate(columns_list):\n",
    "        sns.boxplot(x=data[col], color='#404B69', ax=axs[i])\n",
    "        axs[i].set_title(f'{col} (sesgo: {data[col].skew().round(2)})', fontsize=12)\n",
    "        Q1 = data[col].quantile(0.25)\n",
    "        Q3 = data[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outliers = ((data[col] < (Q1 - 1.5 * IQR)) | (data[col] > (Q3 + 1.5 * IQR)))\n",
    "        outliers_index = data[outliers].index.tolist()\n",
    "        outliers_values = data[col][outliers].tolist()\n",
    "        outliers_df = pd.concat([outliers_df,pd.DataFrame({'Column': col, 'Outlier_index': outliers_index, 'Outlier_values': outliers_values})], ignore_index=True)\n",
    "        axs[i].plot([], [], 'ro', alpha=0.5, label=f'Atípicos: {outliers.sum()}')\n",
    "        axs[i].legend(loc='upper right', fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    return outliers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = pd.DataFrame (df_train, columns= df_train.select_dtypes(include=['int64','float64']).columns)\n",
    "cat_cols = pd.DataFrame (df_train, columns= df_train.select_dtypes(include=['object', 'category']).columns)\n",
    "\n",
    "outliers_df = plot_numerical_features_boxplots(data=df_train, columns_list=num_cols, rows=8, cols=3, title='Diagrama de cajas y bigotes para datos atípicos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificación de datos faltantes\n",
    "missing_values = df_train.isnull().sum()\n",
    "print('Valores faltantes en cada columna:')\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos el porcentaje de valores nulos por columna\n",
    "percent_missing = df_train.isnull().mean() * 100\n",
    "print(percent_missing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_missing_values_heatmap(df, figsize=(20, 6), cmap=\"Blues\"):\n",
    "    \"\"\"\n",
    "    Genera un mapa de calor para visualizar la ubicación de los valores faltantes en un DataFrame.\n",
    "\n",
    "    Parámetros:\n",
    "    - df: DataFrame en el que se buscan los valores faltantes.\n",
    "    - figsize: Tamaño de la figura del mapa de calor (por defecto (20, 6)).\n",
    "    - cmap: El esquema de color para el mapa de calor (por defecto \"Blues\").\n",
    "    \n",
    "    Retorna:\n",
    "    - Un gráfico de mapa de calor que muestra los valores faltantes.\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(df.isnull(), yticklabels=False, cbar=False, cmap=cmap)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llamamos a la funcion con nuestro dataframe df_train\n",
    "plot_missing_values_heatmap(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](Images/EDA-campesino.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONCLUSIONES SOBRE EDA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La variable objetivo 'Target', se encuentra algo desbalanceada teniendo una proporción del 71.5% Normal(0) y 28.5% Anormal (1)\n",
    "- Matemática y gráficamente se observan comportamientos de outliers que se debatirán con especialistas del área médica\n",
    "- Las proporciones de la variable objetivo se comportan o observan de forma similar si las contrastamos contra las variables categóricas, es decir aproximadamente hay valores Normales > 70% en cada categoría de cada variable, y valores < 29% Anormales en cada categoría de cada variable\n",
    "- El análisis de correlaciones nos muestra unos resultados no muy claros por eso haremos uso de un algoritmo para mirar características que mejor describan la variable objetivo\n",
    "- Matemáticamente se hayan una buena cantidad de outliers pero al tratarse de un área medica no podemos despreciarlos o acotarlos a la ligera\n",
    "- Se encuentran algunas variables con una gran porcentaje de valores faltantes o nulos > 70%\n",
    "- Se evidencian 2 variables que deben tener un trato personalizado para su imputación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada columna tiene 23494 regsitros y de las 46(45 independientes y 1 dependiente), tenemos 9 con datos faltantes o nulos.\n",
    "\n",
    "- min_Tiempo_CP_Fliar              23486 faltantes >= 99%\n",
    "- psa_max_gr_flia                  23330 faltantes >= 99%\n",
    "- psa_min_gr_flia                  23330 faltantes >= 99%\n",
    "- IMC                              10364 faltantes >= 44%\n",
    "- AGRUPACION_SISTOLICA              3320 faltantes >= 14%\n",
    "- AGRUPACION_DIASTOLICA             3320 faltantes >= 14%\n",
    "- RIESGOS                          16283 faltantes >= 69%\n",
    "- PERDIDA_DE_PESO                  17723 faltantes >= 75%\n",
    "- CANCER_MAMA_FAMILIAR              6802 faltantes >= 28%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teniendo este panorama decidimos buscar un poco de ayuda con profesionales de area médica para dicidir que camino tomar en la imputación y eliminación de variables llegando a las siguientes conclusiones:\n",
    "\n",
    "1. La variable **CANCER_MAMA_FAMILIAR** la imputadremos con una categoria que indique no ha sido medido o tomado. Esta desición se consultó con el host de la competencia después de ser analizada por el grupo de especialistas obteniedo acuerdo de él también, citamos su respuesta: \"Tienes razón con la imputación de la variable cancer de mama familiar, no sería sano imputar con metodologias de ML, sin embargo, se puede agregar una categoria que indique que el valor no se evidencia\". De acuerdo a como estaba formateada la variable esta nueva categoría es '2'.\n",
    "\n",
    "2. La variable **RIESGOS** será imputada con un valor personalizado 0, debido a un argumento expresado por el host de la competencia: \"Para el caso de los RIESGOS, los valores nulos corresponden a que aun no se les ha identifcado algun riesgo, por lo tanto, se pueden marcar con 0 (cero) riesgos\".\n",
    "\n",
    "3. La variable **IMC** será imputada con una nueva categoria para su valores faltantes, su valor es: 'No_medido'. Aquí un grupo de médico especialistas nos indicó que podría ser un grave error imputar por moda este campo ya que el IMC en la vida y experiencia real del trato de este tipo de cancer si da o brinda información importante a través de las cuales los especialistas pueden identificar de una mejor manera éste pronóstico. \n",
    "\n",
    "4. Las variables **min_Tiempo_CP_Fliar**, **psa_max_gr_flia** y **psa_min_gr_flia** serán eliminadas para la creación de los modelos pues presentan más del 99% de valores faltantes.\n",
    "\n",
    "5. La variable **PERDIDA_DE_PESO** será eliminada para la creación de los modelos debido a que presenta más del 75% de valores faltantes\n",
    "\n",
    "6. Las variables **AGRUPACION_SISTOLICA** y **AGRUPACION_DIASTOLICA** serán imputadas de forma tradicional sin buscar una forma especial debido a la significancia expresada por el grupo de especialistas sobre estas mismas, incluso más adelante se denotará si incluso podrían ser no tenidas en cuenta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preprocesamiento de datos**\n",
    "\n",
    "- Analisis de importancia de variables\n",
    "- Formateo de variables.\n",
    "- Imputar valores faltantes.\n",
    "- Manejo de outliers.\n",
    "- Escalar las variables numéricas (normalización o estandarización).\n",
    "- Codificar las variables categóricas mediante dummificación (One-Hot Encoding).\n",
    "- Considerar técnicas como reducción de dimensionalidad (PCA) si es necesario.\n",
    "- Crear un pipeline de preprocesamiento para que los datos estén listos para usarse en los modelos.\n",
    "\n",
    "Abordaremos esto de una forma donde haremos un preprocesamiento previo a algunas variables que consideramos no alterará ni pondrá en riesgo nuestro dataframe para el conocido **data leakage**, por eso algunos pasos se verán reflejados en el entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Análisis de importancia de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usamos un modelo de random forest para analizar importancia de variables\n",
    "def plot_feature_importances(df, target_var, test_size=0.3, random_state=42):\n",
    "    \"\"\"\n",
    "    Entrena un modelo de Random Forest y genera un gráfico de barras horizontal con la importancia de las variables.\n",
    "\n",
    "    Parámetros:\n",
    "    - df: DataFrame que contiene las características y la variable objetivo.\n",
    "    - target_var: Nombre de la variable objetivo en el DataFrame.\n",
    "    - test_size: Proporción del conjunto de datos para pruebas (predeterminado 0.3).\n",
    "    - random_state: Semilla para la reproducibilidad del modelo (predeterminado 42).\n",
    "    \n",
    "    Retorna:\n",
    "    - Un gráfico de barras horizontal que muestra la importancia de las características.\n",
    "    \"\"\"\n",
    "\n",
    "    # Separar características y variable objetivo\n",
    "    X = df.drop(target_var, axis=1)\n",
    "    y = df[target_var]\n",
    "\n",
    "    # Convertir variables categóricas a numéricas si es necesario\n",
    "    X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "    # Entrenar el modelo Random Forest\n",
    "    model = RandomForestClassifier(random_state=random_state)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Obtener importancias\n",
    "    importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "\n",
    "    # Graficar la importancia de las variables\n",
    "    importances.sort_values().plot(kind='barh', figsize=(8, 18))\n",
    "    plt.title('Importancia de las Variables')\n",
    "    plt.xlabel('Importancia')\n",
    "    plt.ylabel('Características')\n",
    "    plt.show()\n",
    "\n",
    "    return importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = plot_feature_importances(df_train, 'Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_numerical_feature_importances(df, target_var, n_estimators=100, random_state=42, figsize=(8, 6)):\n",
    "    \"\"\"\n",
    "    Entrena un modelo de Random Forest con variables numéricas y genera un gráfico de barras horizontal\n",
    "    con la importancia de las mismas.\n",
    "\n",
    "    Parámetros:\n",
    "    - df: DataFrame que contiene las características numéricas y la variable objetivo.\n",
    "    - target_var: Nombre de la variable objetivo en el DataFrame.\n",
    "    - n_estimators: Número de árboles en el modelo Random Forest (predeterminado 100).\n",
    "    - random_state: Semilla para la reproducibilidad del modelo (predeterminado 42).\n",
    "    - figsize: Tamaño de la figura del gráfico (predeterminado (8, 6)).\n",
    "\n",
    "    Retorna:\n",
    "    - Un gráfico de barras horizontal que muestra la importancia de las variables numéricas.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Seleccionar las variables numéricas\n",
    "    numerical_vars = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    X_num = df[numerical_vars]\n",
    "    y = df[target_var]\n",
    "\n",
    "    # Entrenar el modelo Random Forest\n",
    "    rf = RandomForestClassifier(n_estimators=n_estimators, random_state=random_state)\n",
    "    rf.fit(X_num, y)\n",
    "\n",
    "    # Obtener importancias de las variables\n",
    "    importances = pd.Series(rf.feature_importances_, index=numerical_vars)\n",
    "\n",
    "    # Graficar la importancia de las variables\n",
    "    importances.sort_values().plot(kind='barh', figsize=figsize)\n",
    "    plt.title('Importancia de Variables Numéricas')\n",
    "    plt.xlabel('Importancia')\n",
    "    plt.ylabel('Variables')\n",
    "    plt.show()\n",
    "\n",
    "    return importances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = plot_numerical_feature_importances(df_train, 'Target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos pruebas de Chi-Cuadrado para evaluar la asociación entre cada variable categórica y 'Target'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi2_test_categorical_vars(df, target_var, significance_level=0.05):\n",
    "    \"\"\"\n",
    "    Realiza la prueba de Chi-cuadrado entre las variables categóricas y la variable objetivo.\n",
    "\n",
    "    Parámetros:\n",
    "    - df: DataFrame que contiene las variables categóricas y la variable objetivo.\n",
    "    - target_var: Nombre de la variable objetivo.\n",
    "    - significance_level: Nivel de significancia para determinar asociación significativa (por defecto 0.05).\n",
    "\n",
    "    Retorna:\n",
    "    - Un reporte de la prueba Chi-cuadrado para cada variable categórica.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Seleccionar las variables categóricas\n",
    "    categorical_vars = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    # Iterar sobre las variables categóricas\n",
    "    for var in categorical_vars:\n",
    "        # Crear la tabla de contingencia\n",
    "        contingency_table = pd.crosstab(df[var], df[target_var])\n",
    "\n",
    "        # Realizar la prueba de Chi-cuadrado\n",
    "        chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "        # Mostrar los resultados\n",
    "        print(f'Variable: {var}')\n",
    "        print(f'Estadístico Chi2: {chi2:.2f}, Valor p: {p:.4f}')\n",
    "        if p < significance_level:\n",
    "            print('→ Asociación significativa con Target')\n",
    "        else:\n",
    "            print('→ No hay asociación significativa con Target')\n",
    "        print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_test_categorical_vars(df_train, 'Target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos la información mutua para medir la dependencia entre las variables categóricas y 'Target'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mutual_information_categorical(df, target_var, figsize=(8, 6)):\n",
    "    \"\"\"\n",
    "    Calcula la información mutua entre las variables categóricas y la variable objetivo,\n",
    "    y genera un gráfico de barras horizontal con los resultados.\n",
    "\n",
    "    Parámetros:\n",
    "    - df: DataFrame que contiene las variables categóricas y la variable objetivo.\n",
    "    - target_var: Nombre de la variable objetivo en el DataFrame.\n",
    "    - figsize: Tamaño de la figura del gráfico (por defecto (8, 6)).\n",
    "\n",
    "    Retorna:\n",
    "    - Un gráfico de barras horizontal que muestra la información mutua de las variables categóricas.\n",
    "    \"\"\"\n",
    "\n",
    "    # Seleccionar variables categóricas\n",
    "    categorical_vars = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    # Convertir variables categóricas a códigos numéricos\n",
    "    X_cat = df[categorical_vars].apply(lambda x: x.astype('category').cat.codes)\n",
    "\n",
    "    # Extraer la variable objetivo\n",
    "    y = df[target_var]\n",
    "\n",
    "    # Calcular la información mutua\n",
    "    mi = mutual_info_classif(X_cat, y, discrete_features=True)\n",
    "\n",
    "    # Crear una serie con los resultados de la información mutua\n",
    "    mi_series = pd.Series(mi, index=categorical_vars)\n",
    "\n",
    "    # Graficar los resultados\n",
    "    mi_series.sort_values().plot(kind='barh', figsize=figsize)\n",
    "    plt.title('Información Mutua de Variables Categóricas')\n",
    "    plt.xlabel('Información Mutua')\n",
    "    plt.ylabel('Variables')\n",
    "    plt.show()\n",
    "\n",
    "    return mi_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_series = plot_mutual_information_categorical(df_train, 'Target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos análisis gráficos nos confirman nuestras anteriores conclusiones y las fortalecen. de momento continuaremos con nuestros planteamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Formateo de variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para pruebas\n",
    "df_train_transformed = df_train.copy()\n",
    "df_train_transformed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un Transformer para renombrar y dar formato a las columnas\n",
    "class Rename_columns(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        result = X.copy()\n",
    "\n",
    "        # Aplicamos el formato: eliminamos espacios, convertimos a minúsculas y reemplazamos por guiones bajos\n",
    "        new_cols = []\n",
    "        for col in result.columns:\n",
    "            new_cols.append(re.sub(r'\\s+', ' ', col.strip()).replace(' ', '_').lower())\n",
    "        result.columns = new_cols\n",
    "        \n",
    "        # Renombramos columnas específicas si es necesario\n",
    "        result = result.rename(columns={'estado_civi': 'estado_civil'})  #cambiamos el nombre a esta variable porque parece incompleto\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba\n",
    "df_train_transformed = Rename_columns().fit_transform(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba\n",
    "df_train_transformed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creamos un transformador personalizado que asigne valores específicos a las columnas que lo requieran, manteniendo el pipeline limpio y modular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un Transformer Trabajando con el índice de las variables para imputar unos valores específicos\n",
    "class Custom_imputer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, custom_values=None):\n",
    "        # Definir los valores específicos para las columnas que deseas imputar manualmente si no se pasa custom_values\n",
    "        if custom_values is None:\n",
    "            self.custom_values = {\n",
    "                28: '2',  # CANCER_MAMA_FAMILIAR, agregamos una categoria 2 que indique que el valor no se evidencia.\n",
    "                22: 0,    # RIESGOS, los valores nulos corresponden a que aun no se les ha identifcado algun riesgo, por lo tanto, se pueden marcar con 0 (cero) riesgos.\n",
    "                17: 'No_medido'  # IMC, los valores nulos corresponden a que aun no se les ha identifcado IMC, creamos una nueva categoria para no imputar por moda\n",
    "            }\n",
    "        else:\n",
    "            self.custom_values = custom_values  # Usar el diccionario proporcionado si se pasa uno\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        result = X.copy()\n",
    "        for idx, value in self.custom_values.items():\n",
    "            result.iloc[:, idx].fillna(value, inplace=True)  # Imputar valor específico usando el índice\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba\n",
    "df_train_transformed = Custom_imputer().fit_transform(df_train_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba\n",
    "df_train_transformed[['cancer_mama_familiar', 'riesgos', 'imc']].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba\n",
    "df_train_transformed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creamos un transformador personalizado para eliminar columnas en función del porcentaje de valores faltantes o errados (nulos) que tienen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba Calculamos el porcentaje de valores nulos por columna\n",
    "percent_missing = df_train_transformed.isnull().mean() * 100\n",
    "print(percent_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un Transformer para eliminar variables que tengan más de 75% de datos nulos o faltantes\n",
    "class Drop_columns_by_missing_Values(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold=0.75):  # Umbral en porcentaje, 0.75 = 75%\n",
    "        self.threshold = threshold\n",
    "        self.columns_to_drop = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Calcular el porcentaje de valores faltantes por columna\n",
    "        percent_missing = X.isnull().mean()\n",
    "        # Guardar las columnas que superan el umbral\n",
    "        self.columns_to_drop = percent_missing[percent_missing > self.threshold].index\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # Eliminar las columnas que superan el umbral\n",
    "        return X.drop(columns=self.columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba\n",
    "df_train_transformed = Drop_columns_by_missing_Values().fit_transform(df_train_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba Calculamos el porcentaje de valores nulos por columna\n",
    "percent_missing = df_train_transformed.isnull().mean() * 100\n",
    "print(percent_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos uso de un pipeline para dejar listo nuestros dataframe que posteriormente utilizaremos para entrenar nuestros modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un Pipeline donde reunimos nuestros transformer para hacer todo este primer preproceso en una sola línea\n",
    "pipe_custom_pre = Pipeline(steps = [('rename columns', Rename_columns()),\n",
    "                                    ('custom imputer', Custom_imputer()),\n",
    "                                    ('drop columns > 75%', Drop_columns_by_missing_Values())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba\n",
    "df_train_pipeline_transformed = pipe_custom_pre.fit_transform(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba\n",
    "df_train_pipeline_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba\n",
    "df_train_pipeline_transformed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prueba con los kilos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separar variable objetivo y características\n",
    "X = df_train_pipeline_transformed.drop('target', axis=1)\n",
    "y = df_train_pipeline_transformed['target']\n",
    "\n",
    "# Separar en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocesamiento\n",
    "# Definir las columnas numéricas y categóricas\n",
    "numeric_features = df_train_pipeline_transformed.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = df_train_pipeline_transformed.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Pipeline de preprocesamiento\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Mapeo de valores numéricos a nombres de kernel\n",
    "kernel_map = {0: 'linear', 1: 'rbf', 2: 'poly'}\n",
    "\n",
    "# Función para optimización bayesiana de SVC + PCA\n",
    "def optimize_svc_pca(C, gamma, n_components, kernel_numeric):\n",
    "    \"\"\"Función objetivo para optimización bayesiana del SVC + PCA\"\"\"\n",
    "    # Convertir el número del kernel a su correspondiente nombre\n",
    "    kernel = kernel_map[int(round(kernel_numeric))]\n",
    "\n",
    "    # Crear pipeline del modelo\n",
    "    model_svc = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('pca', PCA(n_components=int(n_components))),  # Número de componentes como parámetro\n",
    "        ('classifier', SVC(probability=True, kernel=kernel, C=C, gamma=gamma if kernel != 'linear' else 'scale'))])\n",
    "\n",
    "    # Validación cruzada y cálculo del AUC\n",
    "    auc = cross_val_score(model_svc, X_train, y_train, cv=5, scoring='roc_auc').mean()\n",
    "    return auc\n",
    "\n",
    "# Definir el espacio de búsqueda para C, gamma, n_components y kernel_numeric\n",
    "param_bounds = {\n",
    "    'C': (0.1, 10),\n",
    "    'gamma': (0.0001, 1),  # gamma se optimiza para todos los kernels (lo manejamos dentro de la función)\n",
    "    'n_components': (2, X_train.shape[1]),  # Número de componentes en PCA\n",
    "    'kernel_numeric': (0, 2)  # Valor numérico para kernel: 0='linear', 1='rbf', 2='poly'\n",
    "}\n",
    "\n",
    "# Optimización bayesiana\n",
    "optimizer = BayesianOptimization(\n",
    "    f=lambda C, gamma, n_components, kernel_numeric: optimize_svc_pca(C, gamma, n_components, kernel_numeric),\n",
    "    pbounds=param_bounds,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "optimizer.maximize(init_points=5, n_iter=10)\n",
    "\n",
    "\"\"\"\n",
    "Procesó por +550 min\n",
    "    hizo 2 iteraciones:\n",
    "        la primera +/- a los 7 minutos\n",
    "        la segunda +/- a los 17 minutos\n",
    "        Con el procesador overclokeado a max 5.2ghz en promedio 4.0 a 5.2 ghz\n",
    "\n",
    "Procedemos a hacer más pruebas e ir de menos a más\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
